Diffusion models have demonstrated remarkable achievements in the domain of text-to-image synthesis, showcasing their ability to creatively merge distinct concepts such as objects, shapes, and textures, thereby producing innovative imagery. This assertion was validated through the application of Stable Diffusion \cite{rombach2022highresolution} for crafting images from diverse textual descriptions, as illustrated in Figure 2.

The Imagen model, introduced by Saharia et al. \cite{saharia2022photorealistic}, represents a method for text-to-image synthesis, incorporating a text encoder and a series of diffusion models for crafting high-resolution images, conditioned on text embeddings produced by the encoder. Additionally, a novel benchmark for text-to-image evaluation named DrawBench was proposed. In terms of architecture, the authors developed Efficient U-Net to enhance efficiency, applying this design within their text-to-image synthesis experiments.

Gu et al. \cite{gu2022vector} put forward the VQ-Diffusion model, a novel approach for text-to-image synthesis that eliminates the unidirectional bias found in earlier methods. Through its unique masking strategy, it prevents error accumulation during inference. The model operates in two phases: initially utilizing a VQ-VAE to represent images as discrete tokens, followed by a discrete diffusion model acting on the VQ-VAE's latent space, guided by caption embeddings. This process is inspired by masked language modeling, replacing some tokens with a [mask] token.

Avrahami et al. \cite{avrahami2022blended} developed a text-conditional diffusion model reliant on CLIP \cite{radford2021learning} embeddings for both image and text. Employing a dual-stage approach, the first generates the image embedding, while the second, acting as a decoder, synthesizes the final image based on both the image embedding and the text caption. To create image embeddings, a diffusion model in the latent space was utilized, with a subjective human evaluation assessing the generative outcomes.

Addressing the slow sampling issue inherent in diffusion models, Zhang et al. \cite{zhang2022fast} introduced a novel discretization strategy that minimizes error and allows for larger step sizes, thereby reducing the number of required sampling steps. Utilizing high-order polynomial extrapolations for the score function and an Exponential Integrator to solve the reverse SDE, they significantly decreased the number of network evaluations needed without compromising the models' generative capabilities.

Shi et al. \cite{shi2022divae} merged a VQ-VAE \cite{oord2017neural} with a diffusion model for image generation. Initiating with the VQ-VAE for encoding, they substituted the decoder with a diffusion model, applying the U-Net architecture \cite{nichol2021improved} and incorporating image tokens into its mid-block.

Expanding on the concepts in Blattmann et al. \cite{blattmann2022retrieval}, Rombach et al. \cite{rombach2022textguided} introduced a modification for crafting artistic images by extracting nearest neighbors in the CLIP \cite{radford2021learning} latent space from a dataset, then guiding the reverse denoising process with these embeddings. Given the shared CLIP latent space for text and images, diffusion can also be text-guided. At inference, an artistic image database substitutes the original, steering the model to generate images reflective of the new dataset's style.

Jiang et al. \cite{jiang2022text2human} unveiled a framework to generate full-body human images with detailed clothing representation from three inputs: a human pose, and text descriptions for both the clothing's shape and texture. The initial phase encodes the shape description into an embedding vector, infusing it into a generative encoder-decoder module for shape mapping. Subsequently, a diffusion-based transformer leverages multiple texture-specific, multi-level codebooks for sampling the texture description's embedded representation, as suggested in VQ-VAE \cite{oord2017neural}. Initially, coarse-level codebook indices are sampled, with finer levels predicted via a feed-forward network, utilizing Sentence-BERT \cite{reimers2019sentencebert} for text encoding.

