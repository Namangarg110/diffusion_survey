Meng et al. \cite{Meng2021SDEdit} explored the application of diffusion models in tasks such as stroke painting, stroke-based editing, and image composition, starting with images that offer some form of guidance. These models preserve the original images' characteristics while smoothing out deformations through a forward diffusion process, and subsequently, a reverse process is applied to denoise these images to produce realistic outputs according to the provided guidance, effectively solving the reverse stochastic differential equation (SDE) without necessitating specialized datasets or modifications in training.

An earlier method for modifying specific image regions based on natural language descriptions was introduced \cite{avrahami2022blended}, where users define regions for editing via a mask. This approach uses CLIP guidance to generate images according to text inputs. However, it was observed that merging the generated output with the original image did not always result in a globally coherent image. To overcome this, a modified denoising process was applied that incorporates the masked latent image with a noisy version of the original image during each iteration.

Advancing this work, Avrahami et al. applied latent diffusion models for localized image editing through text. This method encodes both the image and a dynamically adjustable mask into the latent space, where a diffusion process guided by textual descriptions within the target area takes place \cite{Avrahami2022BlendedLatent}. Inspired by Blended Diffusion \cite{avrahami2022blended}, this technique uniquely combines the masked region in latent space with the contemporaneously noised image before decoding, resulting in enhanced performance and efficiency.

