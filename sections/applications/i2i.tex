Saharia and colleagues \cite{Saharia2022Palette} introduce a unified framework for converting images across different contexts using diffusion models, concentrating on tasks such as colorization, inpainting, extension of image borders, and enhancement of compressed images. This framework remains consistent across these tasks, avoiding the necessity for individual modifications per task. They start by evaluating the effectiveness of L1 versus L2 losses, advocating for the latter due to its ability to increase the variety of generated samples. They also underscore the pivotal role of self-attention mechanisms in generating conditional images.

In an effort to facilitate image translation without corresponding pairs, Sasaki and team \cite{Sasaki2021UNITDDPM} develop a strategy that employs two diffusion models trained in tandem. Each model, through the process of reverse denoising, incorporates feedback from the other model's ongoing outputs. Additionally, they introduce a cycle-consistency loss to refine the training of these models.

Zhao et al. \cite{Zhao2022EGSDE} focus on enhancing the efficacy of image translation models based on diffusion by equally valuing source domain data. They use an energy-based approach that operates on both the source and target domains to guide the stochastic differential equation (SDE) solver. This method produces images that maintain universal features while accurately transferring domain-specific attributes. The model utilizes dual feature extractors, each tailored to a particular domain.

Wang and colleagues \cite{Wang2022Pretraining} leverage the pretrained GLIDE model to create a semantically rich latent space for image generation. By modifying the model's architecture to suit various conditions and then fine-tuning it for specific tasks, they achieve notable improvements. This process involves an initial focus on training a new encoder while keeping the decoder static, followed by concurrent training of both components. Adversarial training techniques and normalization strategies are also applied to further refine the quality of generated images.

Li et al. \cite{Li2022VQBB} propose a novel image translation diffusion model that combines the concepts of Brownian bridges and Generative Adversarial Networks (GANs). Their approach starts with encoding images via a Vector Quantized GAN, followed by a diffusion process that acts as a Brownian bridge in the quantized latent space, facilitating the transition between source and target domains. Subsequently, another VQ-GAN decodes these quantized representations to generate the translated image. Each GAN is trained independently within its respective domain.

Building on their previous research, Wolleb and associates \cite{Wolleb2022MultiTask} enhance their diffusion model by integrating a task-specific model in lieu of the standard classifier. This model enriches the sampling process by incorporating gradients from a network designed for a specific application, demonstrated through either a regression or segmentation task. This approach benefits from the existing diffusion model frameworks, eliminating the necessity for comprehensive retraining except for the component tailored to the specific task.
